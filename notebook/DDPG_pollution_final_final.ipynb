{"cells":[{"cell_type":"markdown","metadata":{"id":"Gb4k1t1xiiXv"},"source":["### Import LSTM Model"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5993,"status":"ok","timestamp":1621929509221,"user":{"displayName":"wang peng","photoUrl":"","userId":"03125119609388965113"},"user_tz":-480},"id":"N7ci-cRcEoG5","outputId":"d1997be7-ab42-4a8d-c2cb-5b8176944762"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"]}],"source":["import datetime\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import matplotlib.ticker as ticker\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split,KFold,cross_val_score as CVS\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import itertools\n","import statsmodels.api as sm\n","from sklearn.metrics import mean_squared_error as MSE,mean_absolute_error as MAE,max_error as ME,r2_score as r2\n","import math\n","\n","#import raw data\n","y_val = pd.read_csv('train_data_111.csv')\n","y_val['date']=pd.to_datetime(y_val['date'])\n","#data process\n","y_val.loc[y_val['LV1211 S01 PN111_BV25']>=65,'LV1211 S01 PN111_BV25']=np.nan\n","y_val['LV1211 S01 PN111_BV25']=y_val['LV1211 S01 PN111_BV25'].interpolate()\n","\n","#interpolation\n","y_val.loc[y_val['LV1281 AS361 M1']==0,'LV1281 AS361 M1']=np.nan\n","y_val['LV1281 AS361 M1']=y_val['LV1281 AS361 M1'].interpolate()\n","y_val.loc[y_val['LV1281 VA307 BB42']==0,'LV1281 VA307 BB42']=np.nan\n","y_val['LV1281 VA307 BB42']=y_val['LV1281 VA307 BB42'].interpolate()\n","y_val.loc[y_val['LV1281 VZ366 BB40']==0,'LV1281 VZ366 BB40']=np.nan\n","y_val['LV1281 VZ366 BB40']=y_val['LV1281 VZ366 BB40'].interpolate()\n","y_val.loc[y_val['LV1281 WK363 BB40-1']==0,'LV1281 WK363 BB40-1']=np.nan\n","y_val['LV1281 WK363 BB40-1']=y_val['LV1281 WK363 BB40-1'].interpolate()\n","y_val.loc[y_val['LV1281 WK363 BB40-2']==0,'LV1281 WK363 BB40-2']=np.nan\n","y_val['LV1281 WK363 BB40-2']=y_val['LV1281 WK363 BB40-2'].interpolate()\n","\n","tihuan = ['LV1281 S05 WU301 BB40','LV1281 S05 WU301 BB40-1','LV1281 S06 WU302 BB40','LV1281 S07 WU311 BB40','LV1281 S08 WU312 BB40','LV1281 S09 WU321 BB40']\n","for item in tihuan:\n","    y_val.loc[y_val[item] == 0, item] = np.nan\n","    y_val[item] = y_val[item].interpolate()\n","\n","tihuan2=['LV1281 S07 WU311 M2','LV1281 S08 WU312 M2','LV1281 S09 WU321 M2','LV1281 S05 WU301 M2','LV1281  S06 WU302 M2']\n","y_val.loc[73149:73541, tihuan2] = np.nan\n","for item in tihuan2:\n","    y_val[item] = y_val[item].interpolate()\n","\n","y_val.loc[29118:29133,['Oven1_NMHC','Oven1_NOx']] = np.nan\n","y_val.loc[37679:37698,['Oven1_NMHC','Oven1_NOx']] = np.nan\n","y_val.loc[57740:57755,['Oven1_NMHC','Oven1_NOx']] = np.nan\n","y_val.loc[70569:70585,['Oven1_NMHC','Oven1_NOx']] = np.nan\n","y_val.loc[78940:78954,['Oven1_NMHC','Oven1_NOx']] = np.nan\n","\n","y_val.loc[24098:24102,['Oven1_NMHC']] = np.nan\n","y_val.loc[24118:24122,['Oven1_NMHC']] = np.nan\n","y_val.loc[74000:74004,['Oven1_NMHC']] = np.nan\n","y_val['Oven1_NMHC']=y_val['Oven1_NMHC'].interpolate()\n","y_val['Oven1_NOx']=y_val['Oven1_NOx'].interpolate()\n","\n","Y_val =y_val.iloc[56886:81076,:].copy()\n","Y_val1 = y_val.iloc[20268:44076,:].copy()\n","\n","# Difference for the linear growth variable\n","dif_col=['LV1281 S04 WR706 BV25-1','LV1281 S01 Energy consumption','LV1281 S02 Energy consumption','LV1281 S03 Energy consumption']\n","for item in dif_col:\n","    ts_diff = np.diff(Y_val[item])\n","    Y_val[item] = np.append([0], ts_diff)\n","    ts_diff = np.diff(Y_val1[item])\n","    Y_val1[item] = np.append([0], ts_diff)\n","\n","# Two extracted time periods Y_val1 and Y_val1 are grouped together as the input data set\n","df1 = pd.concat([Y_val1, Y_val], ignore_index=True)\n","bxg=['LA1251 summary AmereMin charge','LV1211 S01 PN212_BV25','LV1211 S01 PN213_BV25','LV1281 AS361 M1','LV1281 VA307 M3','LV1281 WK363 BV25','LV1281 WK363 M2']\n","df1=df1.drop(columns=bxg)\n","\n","# 17 variables selected and the current number of drying rooms\n","plot_variable=['LV1281 S04 WA506 BB40-5', 'LV1281 VZ366 BB40',\n","       'LV1281  S06 WU302 M2', 'LV1281 VA307 BB42',\n","        'LV1281 S04 WA506 BB40-2',\n","       'LV1281 S08 WU312 M2', 'LV1281 S04 WA506 BB40-4',\n","       'LV1211 S02 PR310_BN20', 'LV1281 S07 WU311 M2',\n","       'LV1281 S05 WU301 BB40-1','LV1211 S01 PN111_BL50',\n","       'LV1281 S09 WU321 M2',\n","       'LV1281 S05 WU301 M2', 'LV1211 S02 PR313_BB40-1',\n","        'LV1211 S01 PN111_BV25',\n","       'LV1211 S04 VZ503_M1_F', 'LV1211 S01 PR114_BV25',\n","       'LF1921 S01 TF270 Number of unit','Oven1_NMHC','Oven1_NOx']\n","df1=df1[plot_variable]\n","# Take the first 30 minutes and the first 90 to 60 minutes as input\n","def feature_engineer(df,names):\n","    N=30\n","    look_back=25\n","    skip = 60\n","    N2=30\n","    M=20\n","    lag_cols = [column for column in df][0:]\n","    shift_range = [x + 1 for x in range(N)]\n","    \n","    for i in shift_range:\n","        for col in lag_cols:\n","            new_col='{}_lag_{}'.format(col, i)   \n","            df[new_col]=df[col].shift(i)\n","    shift_range_y = [x + 1 for x in range(M)]\n","    for name in names:\n","        for i in shift_range_y: \n","            new_col = '{}_pre_{}'.format(name, -i)\n","            df[new_col] = df[name].shift(-i)\n","    return df[look_back*skip:-M-1]\n","df = feature_engineer(df1,['Oven1_NMHC','Oven1_NOx'])\n","#2\n","continuous = [i for i in df.loc[:,df.nunique()>2]][0:]\n","categorical = [i for i in df.loc[:,df.nunique()<=2]]\n","#split train and test dataset\n","test_size = 0.3\n","num_test = int(test_size * len(df))\n","num_train = len(df)-num_test\n","train = df[:num_train]\n","test = df[num_train:]\n","\n","\n","scaler = StandardScaler().fit(train[continuous])\n","train_scaled = scaler.transform(train[continuous])\n","test_scaled = scaler.transform(test[continuous])\n","\n","# Convert the numpy array back into pandas dataframe\n","train_scaled = pd.DataFrame(train_scaled, columns=continuous)\n","test_scaled = pd.DataFrame(test_scaled, columns=continuous)\n","\n","# Obtain processed training set and test set data\n","train.reset_index(drop=True, inplace=True)\n","train_scaled.reset_index(drop=True, inplace=True)\n","temp_train = pd.concat([train[categorical],train_scaled],axis=1,join='inner')\n","\n","test.reset_index(drop=True, inplace=True)\n","test_scaled.reset_index(drop=True, inplace=True)\n","temp_test = pd.concat([test[categorical],test_scaled],axis=1,join='inner')\n","\n","X_train_scaled = temp_train.iloc[:,0:-40]\n","Y_train_scaled = temp_train.iloc[:,-40:]\n","X_test_scaled = temp_test.iloc[:,0:-40]\n","Y_test_scaled = temp_test.iloc[:,-40:]"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1513,"status":"ok","timestamp":1621929513171,"user":{"displayName":"wang peng","photoUrl":"","userId":"03125119609388965113"},"user_tz":-480},"id":"plp_ags0iiYL"},"outputs":[],"source":["scaler_minmax1 = MinMaxScaler().fit(train[continuous].iloc[:,0:-40])\n","scaler_std1= StandardScaler().fit(train[continuous].iloc[:,0:-40])\n","\n","scaler_std2 = StandardScaler().fit(train[continuous].iloc[:,-40:])"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":799,"status":"ok","timestamp":1621929517049,"user":{"displayName":"wang peng","photoUrl":"","userId":"03125119609388965113"},"user_tz":-480},"id":"4HMFuIKdiiYN"},"outputs":[],"source":["train_observation=train.iloc[:,0:620]\n","train_observation=train_observation.values\n","observation_high=train_observation.max(axis=0)\n","observation_low=train_observation.min(axis=0)"]},{"cell_type":"markdown","metadata":{"id":"3QFHYLr3iiYO"},"source":["### Train deep rainforcement learning model"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2312,"status":"ok","timestamp":1621929523942,"user":{"displayName":"wang peng","photoUrl":"","userId":"03125119609388965113"},"user_tz":-480},"id":"brZ6gO0LiiYP"},"outputs":[],"source":["from keras.models import load_model\n","\n","model_NMHC = load_model('lstm_model_NMHC.h5')\n","model_NOx = load_model('lstm_model_NOx.h5')\n","data_mean=[0.71,42.8]\n","data_std=[0.43,10.5]"]},{"cell_type":"markdown","metadata":{"id":"QIwATmEcm_kG"},"source":["Define objective function"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1621929523944,"user":{"displayName":"wang peng","photoUrl":"","userId":"03125119609388965113"},"user_tz":-480},"id":"8Dl_8ymzIRW_"},"outputs":[],"source":["class ObjectiveFunction(object):\n","    @staticmethod\n","    def emission(X):\n","        X=np.array(X)\n","\n","#         X=scaler_minmax1.transform(X.reshape(1, -1))\n","        X=scaler_std1.transform(X.reshape(1, -1))#normalize\n","        train_scaled = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n","        y_NMHC=model_NMHC.predict(train_scaled)[0]\n","        y_NOx=model_NOx.predict(train_scaled)[0]\n","        #Disnormalize the resulting predicted value\n","        y_std= np.append(y_NMHC,y_NOx, axis=0)\n","        y=scaler_std2.inverse_transform(y_std.reshape(1, -1))\n","        return y[0]#returned the amount of  pollutants emitted over the next 20 minutes\n","\n","    @staticmethod\n","    # N is the number of vehicles in the drying room\n","    def weighted_sum(y,N):#传入scaler_std2\n","        # Rewards associated with NOx emissions\n","        y_NMHC=np.mean(y[0:20])\n","        y_NOx=np.mean(y[20:])\n","        if y_NOx<data_mean[1]:\n","            if N==0:\n","                reward1 = (N+1)/(data_std[1]/(data_mean[1]-y_NOx));\n","                reward1 =reward1*100;\n","            else:\n","                reward1 = N/(data_std[1]/(data_mean[1]-y_NOx));\n","                reward1 =reward1*100;\n","        else:\n","            if N==0:\n","                reward1 = -(1000/(N+1))*(1/(data_std[1]/(y_NOx-data_mean[1])));\n","                reward1 =reward1*100;\n","            else:\n","                reward1 = -(1000/N)*(1/(data_std[1]/(y_NOx-data_mean[1])));\n","                reward1 =reward1*100;  \n","\n","        #Rewards associated with NMHC emissions\n","        if y_NMHC<data_mean[0]:\n","            if N==0:\n","                reward2 = (N+1)/(data_std[0]/(data_mean[0]-y_NMHC));\n","                reward2 =reward2*100;\n","            else:\n","                reward2 = N/(data_std[0]/(data_mean[0]-y_NMHC));\n","                reward2 =reward2*100;\n","        else:\n","            if N==0:\n","                reward2 = -(100/(N+1))*(1/(data_std[0]/(y_NMHC-data_mean[0])));\n","                reward2 =reward2*100;\n","            else:\n","                reward2 = -(100/N)*(1/(data_std[0]/(y_NMHC-data_mean[0])));\n","                reward2 =reward2*100;\n","        # Do not exceed the limits of prescribed pollutant discharge\n","        if y_NOx>123 or y_NMHC>1.75:\n","            reward3=-100;\n","        else:\n","            reward3=0\n","\n","        reward=reward1+reward2+reward3\n","        return reward"]},{"cell_type":"markdown","metadata":{"id":"MwqmKoGeiiYV"},"source":["DDPG defination"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":425,"status":"ok","timestamp":1621929695452,"user":{"displayName":"wang peng","photoUrl":"","userId":"03125119609388965113"},"user_tz":-480},"id":"UUiDo72hiiYX"},"outputs":[],"source":["import argparse\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--env_name', default='RLWWTP-v0')\n","parser.add_argument('--tau', default=0.005, type=float)\n","parser.add_argument('--target_update_interval', default=1, type=int)\n","parser.add_argument('--learning_rate', default=1e-2, type=float)\n","parser.add_argument('--gamma', default=0.99, type=float)\n","parser.add_argument('--capacity', default=3000, type=int)\n","parser.add_argument('--capacity2', default=600, type=int)\n","\n","parser.add_argument('--batch_size', default=128, type=int)\n","parser.add_argument('--seed', default=False, type=bool)\n","parser.add_argument('--random_seed', default=10, type=int)\n","\n","parser.add_argument('--log_interval', default=50, type=int)\n","parser.add_argument('--load', default=False, type=bool)\n","parser.add_argument('--max_episode', default=100, type=int)\n","parser.add_argument('--max_length_of_trajectory', default=1200, type=int)  # length of each trajectory,1200是期望\n","parser.add_argument('--print_log', default=1, type=int)\n","parser.add_argument('--update_iteration', default=10, type=int)\n","args = parser.parse_args(args=[])"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2677,"status":"ok","timestamp":1621929557723,"user":{"displayName":"wang peng","photoUrl":"","userId":"03125119609388965113"},"user_tz":-480},"id":"zRFBisbQQvNc","outputId":"585ccef6-d42f-4ee8-a3d1-393f2d5d69e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (56.1.0)\n"]}],"source":["!pip install tensorboardX"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":329,"status":"ok","timestamp":1621929707625,"user":{"displayName":"wang peng","photoUrl":"","userId":"03125119609388965113"},"user_tz":-480},"id":"RHlEfce6agXJ"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.nn import init\n","from tensorboardX import SummaryWriter\n","\n","args = parser.parse_args([])\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","torch.cuda.empty_cache()\n","\n","\n","class ReplayBuffer:\n","    def __init__(self, max_size=args.capacity):\n","        self.storage = []\n","        self.max_size = max_size\n","        self.ptr = 0\n","\n","    def push(self, data):\n","        if len(self.storage) == self.max_size:  # if buffer is full, cover old data.\n","            self.storage[int(self.ptr)] = data\n","            self.ptr = (self.ptr + 1) % self.max_size\n","        else:\n","            self.storage.append(data)\n","\n","    def sample(self, batch_size):\n","        ind = np.random.randint(0, len(self.storage), size=batch_size)\n","        state_buffer, next_state_buffer, action_buffer, reward_buffer, done_buffer = [], [], [], [], []\n","\n","        for i in ind:\n","            state, next_state, action, reward, done = self.storage[i]\n","            state_buffer.append(np.array(state, copy=False))\n","            next_state_buffer.append(np.array(next_state, copy=False))\n","            action_buffer.append(np.array(action, copy=False))\n","            reward_buffer.append(np.array(reward, copy=False))\n","            done_buffer.append(np.array(done, copy=False))\n","\n","        return np.array(state_buffer), np.array(next_state_buffer), np.array(action_buffer), \\\n","               np.array(reward_buffer).reshape(-1, 1), np.array(done_buffer).reshape(-1, 1)\n","\n","\n","class Actor(nn.Module):\n","    def __init__(self, state_dim, action_dim, max_action, min_action):\n","        super(Actor, self).__init__()\n","\n","        self.l1 = nn.Linear(state_dim, 64)\n","        self.l2 = nn.Linear(64, 128)\n","        self.l3 = nn.Linear(128, 64)\n","        self.l4 = nn.Linear(64, action_dim)\n","\n","        self.max_action = torch.from_numpy(max_action).to(device)\n","        self.min_action = torch.from_numpy(min_action).to(device)\n","\n","    def forward(self, x):\n","        x = F.relu(self.l1(x))\n","        x = F.relu(self.l2(x))\n","        x = F.relu(self.l3(x))\n","        #x = self.max_action * torch.sigmoid(self.l4(x))\n","        x = torch.sigmoid(self.l4(x))\n","        return x\n","\n","\n","class Critic(nn.Module):\n","    def __init__(self, state_dim, action_dim):\n","        super(Critic, self).__init__()\n","        self.l1 = nn.Linear(state_dim + action_dim, 64)\n","        self.l2 = nn.Linear(64, 128)\n","        self.l3 = nn.Linear(128, 64)\n","        self.l4 = nn.Linear(64, 1)\n","\n","    def forward(self, x, u):\n","#         a = torch.cat([x, u], 1).unsqueeze(0)\n","#         x = F.relu(self.l1(a))\n","        x = F.relu(self.l1(torch.cat([x, u], 1)))\n","        x = F.relu(self.l2(x))\n","        x = F.relu(self.l3(x))\n","        x = self.l4(x)\n","        return x\n","\n","\n","class DDPG(object):\n","    def __init__(self, state_dim, action_dim, max_action, min_action, phase=1):\n","        if phase != 1:\n","            self.actor = torch.load('./model/actor.pkl', map_location='cuda')\n","            self.actor.to(device)\n","        elif phase == 1:\n","            self.actor = Actor(state_dim, action_dim, max_action, min_action).to(device)\n","        self.actor_target = Actor(state_dim, action_dim, max_action, min_action).to(device)\n","\n","        self.actor_target.load_state_dict(self.actor.state_dict())\n","        self.actor_optimizer = optim.Adam(self.actor.parameters(),args.learning_rate)\n","\n","        if phase != 1:\n","            self.critic = torch.load('./model/critic.pkl', map_location='cuda')\n","            self.critic.to(device)\n","        elif phase == 1:\n","            self.critic = Critic(state_dim, action_dim).to(device)\n","        self.critic_target = Critic(state_dim, action_dim).to(device)\n","        self.critic_target.load_state_dict(self.critic.state_dict())\n","        self.critic_optimizer = optim.Adam(self.critic.parameters(), args.learning_rate)\n","\n","        self.replay_buffer = ReplayBuffer()\n","        self.writer = SummaryWriter('./log/')\n","        self.num_critic_update_iteration = 0\n","        self.num_actor_update_iteration = 0\n","        self.num_training = 0\n","\n","    def selection_action(self, state):\n","        # state = torch.FloatTensor(np.array(state).reshape(1,-1)).to(device)\n","        state = torch.FloatTensor(np.array(state)).to(device)\n","        return self.actor(state).cpu().detach().numpy()\n","\n","    def update(self, agents):\n","        for it in range(args.update_iteration):\n","            state, next_state, action, reward, done = self.replay_buffer.sample(args.batch_size)\n","            state = torch.FloatTensor(state).to(device)\n","            # state = torch.FloatTensor(state).to(device)\n","            # action = torch.FloatTensor(action).squeeze(12).to(device)\n","            action = torch.FloatTensor(action).to(device)\n","            # next_state = torch.FloatTensor(next_state.reshape(-1,1)).to(device)\n","            next_state = torch.FloatTensor(next_state).to(device)\n","            done = torch.FloatTensor(done).to(device)\n","            reward = torch.FloatTensor(reward).to(device)\n","\n","            #update target_network\n","            #Use target actor to get the corresponding output based on the corresponding input\n","            # x0=agents.actor_target(next_state)\n","\n","            # target_action = x0.float()\n","            x0=self.actor_target(next_state)\n","            target_Q = self.critic_target(next_state, self.actor_target(next_state))\n","            target_Q = reward + ((1 - done) * args.gamma * target_Q).detach()\n","            current_Q = self.critic(state, action)\n","\n","            # update critic network\n","            critic_loss = F.mse_loss(current_Q, target_Q)\n","            self.writer.add_scalar('Loss/critic_loss', critic_loss, global_step=self.num_critic_update_iteration)\n","            self.critic_optimizer.zero_grad()\n","            critic_loss.backward()\n","            self.critic_optimizer.step()\n","            # update actor network\n","            # x1=agents.actor(state)\n","\n","            # cur_action = x1.float()\n","            # actor_loss = -self.critic(state, cur_action).mean()\n","            actor_loss = -self.critic(state, self.actor(state)).mean()\n","            self.writer.add_scalar('Loss/actor_loss', actor_loss, global_step=self.num_actor_update_iteration)\n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            self.actor_optimizer.step()\n","\n","            # update target network\n","            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n","            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n","\n","            self.num_actor_update_iteration += 1\n","            self.num_critic_update_iteration += 1\n","            self.writer.close()"]},{"cell_type":"markdown","metadata":{"id":"JFOrlrb1dCGd"},"source":["environment"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1050,"status":"ok","timestamp":1621929752192,"user":{"displayName":"wang peng","photoUrl":"","userId":"03125119609388965113"},"user_tz":-480},"id":"ryT-ptOkVffG"},"outputs":[],"source":["import logging\n","import math\n","import gym\n","from gym import spaces\n","from gym.utils import seeding\n","import numpy as np\n","import time\n","from utils.tool import clear_text,txt_read\n","import torch\n","import pandas as pd\n","import random\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class RLWWTP(gym.Env):\n","    \n","    metadata = {'render.modes': ['human', 'rgb_array'],'video.frames_per_second': 50 }\n","  \n","    def __init__(self):\n","        # state parameters\n","        self.state_size=620\n","        self.observation_high = observation_high\n","        self.observation_low = observation_low\n","\n","        #action parameters\n","        self.action_size=17\n","        self.action_high = np.array([475,32.37,100,54,164,36,635,97.3,100,310,1000,50,35,43,61.8,44,72])\n","        self.action_low = np.array([376,9.67,60,18,152,9,580,92,75,200,600,20,0,38,59.7,44,0])\n","        \n","        self.action_high2 = np.array([150,36,1,43,100,1,200,96,1,150,856,1,1,43,61.7,45,0.029])\n","        self.action_low2 = np.array([50,18,0,25,31,0,55,0,0,40,466,0,0,26,60.9,0,0])\n","        \n","        # The action space is determined based on the maximum and minimum values of the defined action\n","        # Two action Spaces are set respectively for two agents\n","        self.action_space = spaces.Box(low=self.action_low, high=self.action_high, dtype=np.float32)#spaces.Box创造动作的连续区间\n","        self.action_space2 = spaces.Box(low=self.action_low2, high=self.action_high2, dtype=np.float32)#spaces.Box创造动作的连续区间\n","        # define state space\n","        self.observation_space = spaces.Box(low=self.observation_low, high=self.observation_high, dtype=np.float32)#spaces.Box创造observation的连续空间\n","        # Initialize the environment state\n","        self.state_array=np.array(train[continuous].iloc[5300,0:-40])\n","        self.phase_var=0\n","        self.is_low1=False\n","        self.is_low2=False\n","        self.is_low3=False\n","        self.is_shake1=True\n","        self.is_shake2=True\n","        self.is_shake3=True\n","        self.count=0\n","        self.amount=0\n","        self.seed()\n","#     self.reset()\n","    def seed(self, seed=None):\n","        self.np_random, seed = seeding.np_random(seed)\n","        return [seed]\n","\n","    def step(self, actions,y,count,current_car):\n","\n","        ################################### According to the current number of vehicles, determine which agent selected the action, and the corresponding anti-normalization\n","        actions=actions\n","        if current_car>5:\n","            for i in range(len(actions)):\n","                actions[i]=self.action_low[i]+actions[i]*(self.action_high[i]-self.action_low[i])         \n","        else:\n","            for i in range(len(actions)):\n","                actions[i]=self.action_low2[i]+actions[i]*(self.action_high2[i]-self.action_low2[i])\n","                \n","        #np.savetxt(action_path_1)\n","        # Five environmental states, phase_var=0 corresponds to normal production; phase_var=1 corresponds to production repair state 1; phase_var=2 corresponds to production refurbishment state 2; phase_var=3 corresponds to production repair state 3;\n","        # phase_var=4 corresponds to the shutdown process. phase_var=5 indicates that production is resumed after shutdown\n","        # 0: indicates the environment change during normal production\n","        rand1=random.uniform(0,1)\n","        if self.phase_var==0:\n","            # rand1<0.006, Enter the production finishing section\n","            if rand1<0.006:\n","                self.count+=1\n","                # First three rounds of production refurbishment before entering the shutdown\n","                if self.count<4:\n","                    p=np.array([1/3,1/3,1/3])\n","                    self.phase_var=np.random.choice([1,2,3],p=p.ravel())# Randomly enter one of the production repair states\n","                else:\n","                    self.phase_var=4;\n","\n","            else:\n","            # \n","                if self.state==21:\n","                  # Choose with a certain probability 1，0，-1\n","                    p=np.array([0.05,0.9,0.05])\n","                    self.state+=np.random.choice([1,0,-1],p=p.ravel())\n","                # If st is not equal to 21, return to 21\n","                elif self.state<21:self.state+=1;\n","                else :self.state-=1;\n","        # 1: Environment change in repair state 1\n","        elif self.phase_var==1:\n","          # First descent\n","            if self.is_shake1==True:\n","            # Has not yet reached the lowest point 9\n","                if self.state>9 and self.is_low1==False:\n","                    p=np.array([1/15,1/3,0.6])\n","                    self.state+=np.random.choice([1,0,-1],p=p.ravel())\n","                    # Determine if the bottom has been reached\n","                    if self.state==9:\n","                        self.is_low1=True;   \n","                elif self.is_low1==True and self.state==9:\n","                    # The number of vehicles oscillates around 9, generates a random number, and jumps out of the oscillating process with a certain probability\n","                    p=np.array([0.25,0.5,0.25])\n","                    self.state+=np.random.choice([1,0,-1],p=p.ravel())\n","                    rand2=random.uniform(0,1)\n","                    if rand2<=0.1:\n","                        self.is_shake1=False;# downward oscillation, and the number of vehicles begins to increase\n","\n","                else:\n","                  # the number of vehicles return to 9\n","                    if self.state<9: self.state+=1;\n","                    else:self.state-=1;\n","                    rand3=random.uniform(0,1)\n","                    if rand3<=0.1:\n","                        self.is_shake1=False\n","\n","            else:\n","                # The number of cars has not reached 21\n","                if self.state<21:\n","                    p=np.array([0.6,1/3,1/15])\n","                    self.state+=np.random.choice([1,0,-1],p=p.ravel())\n","\n","                else:\n","                    self.phase_var=0# Return to normal production\n","                    self.is_shake1=True;# Restore parameters to default\n","                    self.is_low1=False\n","\n","        elif self.phase_var==2:# Production refurbishment state 2 Changes in the number of bodies\n","            if self.is_shake2==True:\n","                if self.state>5 and self.is_low2==False:\n","                    p=np.array([1/15,1/3,0.6])\n","                    self.state+=np.random.choice([1,0,-1],p=p.ravel())\n","                    if self.state==5:\n","                        self.is_low2=True;\n","\n","                elif self.is_low2==True and self.state==5:\n","                    p=np.array([0.25,0.5,0.25])\n","                    self.state+=np.random.choice([1,0,-1],p=p.ravel())\n","                    rand2=random.uniform(0,1)\n","                    if rand2<=0.1:\n","                        self.is_shake2=False;\n","\n","                else:\n","                    if self.state<5: self.state+=1;\n","                    else:self.state-=1;\n","                    rand3=random.uniform(0,1)\n","                    if rand3<=0.1:\n","                        self.is_shake2=False\n","\n","            else:\n","                if self.state<21:\n","                    p=np.array([0.6,1/3,1/15])\n","                    self.state+=np.random.choice([1,0,-1],p=p.ravel())\n","                else:\n","                    self.phase_var=0\n","                    self.is_shake2=True;\n","                    self.is_low2=False\n","\n","        elif self.phase_var==3:        \n","            if self.is_shake3==True:\n","                if self.state>15 and self.is_low3==False:\n","                    p=np.array([1/15,1/3,0.6])\n","                    self.state+=np.random.choice([1,0,-1],p=p.ravel())\n","                    if self.state==15:\n","                        self.is_low3=True;\n","\n","                elif self.is_low3==True and self.state==15:\n","                    p=np.array([0.25,0.5,0.25])\n","                    self.state+=np.random.choice([1,0,-1],p=p.ravel())\n","                    rand2=random.uniform(0,1)\n","                    if rand2<=0.1:\n","                        self.is_shake3=False;\n","\n","                else:\n","                    if self.state<15: self.state+=1;\n","                    else:self.state-=1;\n","                    rand3=random.uniform(0,1)\n","                    if rand3<=0.1:\n","                        self.is_shake3=False\n","\n","            else:\n","                if self.state<21:\n","                    p=np.array([0.6,1/3,1/15])\n","                    self.state+=np.random.choice([1,0,-1],p=p.ravel())\n","\n","                else:\n","                    self.phase_var=0\n","                    self.is_shake3=True;\n","                    self.is_low3=False\n","        elif self.phase_var==4:\n","            if self.state>0:\n","                p=np.array([1/15,1/3,0.6])\n","                self.state+=np.random.choice([1,0,-1],p=p.ravel())\n","\n","            else:\n","                rand4=random.uniform(0,1)\n","                if rand4<0.002:\n","                    self.phase_var=5\n","\n","        else:\n","            if self.state<21:\n","                p=np.array([0.6,1/3,1/15])\n","                self.state+=np.random.choice([1,0,-1],p=p.ravel()) \n","            else:\n","                self.phase_var=0\n","                \n","        state=self.state\n","        actions_tmp=list(actions)\n","        actions_tmp.append(state)\n","        actions_tmp.append(y[0][0])\n","        actions_tmp.append(y[0][20])\n","        # Update the status\n","        self.state_array= np.append(actions_tmp,self.state_array, axis=0)\n","        self.state_array=self.state_array[:len(self.state_array)-20]\n","        # Check whether the changed state exceeds the boundary\n","        for i in range(len(self.state_array)):\n","            if self.state_array[i] < self.observation_low[i]: self.state_array[i] = self.observation_low[i]\n","            if self.state_array[i] > self.observation_high[i]: self.state_array[i] = self.observation_high[i]\n","                \n","        reward_1 = self.reward_calculate(self.state_array)\n","        \n","        ddpg_selection_result={\"State\":self.state,\"Reward\":reward_1,\"LV1281 S04 WA506 BB40-5\":actions_tmp[0],'LV1281 VZ366 BB40':actions_tmp[1],\n","                                 'LV1281  S06 WU302 M2':actions_tmp[2],'LV1281 VA307 BB42':actions_tmp[3],\n","                                  'LV1281 S04 WA506 BB40-2':actions_tmp[4],'LV1281 S08 WU312 M2':actions_tmp[5],\n","                                  'LV1281 S04 WA506 BB40-4':actions_tmp[6],'LV1211 S02 PR310_BN20':actions_tmp[7],\n","                                  'LV1281 S07 WU311 M2':actions_tmp[8],'LV1281 S05 WU301 BB40-1':actions_tmp[9],\n","                                  'LV1211 S01 PN111_BL50':actions_tmp[10],'LV1281 S09 WU321 M2':actions_tmp[11],\n","                                  'LV1281 S05 WU301 M2':actions_tmp[12],'LV1211 S02 PR313_BB40-1':actions_tmp[13],\n","                                  'LV1211 S01 PN111_BV25':actions_tmp[14],'LV1211 S04 VZ503_M1_F':actions_tmp[15],\n","                                  'LV1211 S01 PR114_BV25':actions_tmp[16],'LF1921 S01 TF270 Number of unit':actions_tmp[17],\n","                                  'Oven1_NMHC':actions_tmp[18],'Oven1_NOx':actions_tmp[19]}\n","\n","        df=pd.DataFrame(data=ddpg_selection_result,index=[count])\n","        df.to_csv(\"ddpgResult7.csv\", encoding=\"utf-8\", mode=\"a\", header=False)\n","        return self.state_array, reward_1, False, {}\n","    def reward_calculate(self,X):\n","        y=ObjectiveFunction.emission(X)\n","        reward=ObjectiveFunction.weighted_sum(y,self.state)\n","        return reward\n","\n","    def reset(self):\n","        # Game initialization\n","        self.state=random.randint(20, 22)\n","        self.phase_var =0\n","        self.is_low1=False\n","        self.is_low2=False\n","        self.is_low3=False\n","        self.is_shake1=True\n","        self.is_shake2=True\n","        self.is_shake3=True\n","        self.count=0;\n","        self.amount=0\n","        state = self.state\n","        self.state_array=np.array(train[continuous].iloc[5300,0:-40])\n","        return self.state_array"]},{"cell_type":"markdown","metadata":{"id":"kSZ_aPTtHM66"},"source":["Parameters of MADDPG：  \n","state_dim, action_dim, max_action, min_action, actor_path='', critic_path=''，phase=1, actor_lr=1e-3, critic_lr=1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HcPDP8YoYPFT","outputId":"438adcd6-dca3-44a2-a3c9-ffd9aa4a3512"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.8.1+cu101\n","Recursion 0\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"]},{"name":"stdout","output_type":"stream","text":["Ep_i \t0, the ep_r is \t-2302.42, the step is \t1200\n","Ep_i \t1, the ep_r is \t3297.62, the step is \t1200\n","Ep_i \t2, the ep_r is \t-3238.31, the step is \t1200\n","Ep_i \t3, the ep_r is \t4189.64, the step is \t1200\n","Ep_i \t4, the ep_r is \t2770.67, the step is \t1200\n","Ep_i \t5, the ep_r is \t4479.95, the step is \t1200\n","Ep_i \t6, the ep_r is \t2756.23, the step is \t1200\n","Ep_i \t7, the ep_r is \t2822.71, the step is \t1200\n","Ep_i \t8, the ep_r is \t4410.66, the step is \t1200\n","Ep_i \t9, the ep_r is \t2053.35, the step is \t1200\n","Ep_i \t10, the ep_r is \t4237.50, the step is \t1200\n","Ep_i \t11, the ep_r is \t1985.84, the step is \t1200\n","Ep_i \t12, the ep_r is \t3292.33, the step is \t1200\n","Ep_i \t13, the ep_r is \t3445.27, the step is \t1200\n","Ep_i \t14, the ep_r is \t3224.19, the step is \t1200\n","Ep_i \t15, the ep_r is \t1827.13, the step is \t1200\n","Ep_i \t16, the ep_r is \t1316.68, the step is \t1200\n","Ep_i \t17, the ep_r is \t1431.95, the step is \t1200\n","Ep_i \t18, the ep_r is \t3855.34, the step is \t1200\n","Ep_i \t19, the ep_r is \t3293.80, the step is \t1200\n","Ep_i \t20, the ep_r is \t3208.56, the step is \t1200\n","Ep_i \t21, the ep_r is \t2368.65, the step is \t1200\n","Ep_i \t22, the ep_r is \t2653.23, the step is \t1200\n","Ep_i \t23, the ep_r is \t3201.09, the step is \t1200\n","Ep_i \t24, the ep_r is \t3345.77, the step is \t1200\n","Ep_i \t25, the ep_r is \t2454.96, the step is \t1200\n","Ep_i \t26, the ep_r is \t3097.17, the step is \t1200\n","Ep_i \t27, the ep_r is \t2869.02, the step is \t1200\n","Ep_i \t28, the ep_r is \t2377.76, the step is \t1200\n","Ep_i \t29, the ep_r is \t2391.43, the step is \t1200\n","Ep_i \t30, the ep_r is \t3792.67, the step is \t1200\n","Ep_i \t31, the ep_r is \t1995.51, the step is \t1200\n","Ep_i \t32, the ep_r is \t1861.61, the step is \t1200\n"]}],"source":["import os\n","import gym\n","import numpy as np\n","import torch\n","from utils.logger import save_output, concatenate_data\n","from utils.plot import action_plot, reward_plot\n","\n","print(torch.__version__)\n","args = parser.parse_args([])\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","env = RLWWTP();\n","\n","# random seed generation\n","if args.seed:\n","    env.seed(args.random_seed)\n","    torch.manual_seed(args.random_seed)\n","    np.random.seed(args.random_seed)\n","\n","# basic info\n","state_dim = 620\n","action_dim = 17\n","max_action = env.action_space.high\n","min_action = env.action_space.low\n","\n","max_action_dosage = env.action_space2.high\n","min_action_dosage = env.action_space2.low\n","\n","min_val = torch.tensor(1e-7).float().to(device)\n","\n","def main(i, transfer=False):\n","    count=0\n","    episode_reward = []\n","    episode_aer = []\n","    lr = args.learning_rate\n","    agent_do = DDPG(state_dim, action_dim, max_action, min_action, phase=1,)  # initialize ddpg\n","    agent_dosage = DDPG(state_dim, action_dim, max_action_dosage, min_action_dosage, phase=1,)  # initialize ddpg\n","    ep_r = 0\n","    ep_a = []\n","    if transfer:\n","        #agent_do\n","        for name, param in agent_do.actor.named_parameters():\n","            if name.startswith('l4') or name.startswith('l3') or name.startswith('l2'):\n","                param.requires_grad = True\n","            else:\n","                param.requires_grad = False\n","\n","        for name, param in agent_do.critic.named_parameters():\n","            if name.startswith('l4') or name.startswith('l3') or name.startswith('l2'):\n","                param.requires_grad = True\n","            else:\n","                param.requires_grad = False\n","        #agent_dosage       \n","        for name, param in agent_dosage.actor.named_parameters():\n","            if name.startswith('l4') or name.startswith('l3') or name.startswith('l2'):\n","                param.requires_grad = True\n","            else:\n","                param.requires_grad = False\n","\n","        for name, param in agent_dosage.critic.named_parameters():\n","            if name.startswith('l4') or name.startswith('l3') or name.startswith('l2'):\n","                param.requires_grad = True\n","            else:\n","                param.requires_grad = False\n","                \n","    for epoch in range(args.max_episode):\n","        state_do = env.reset()\n","        t=0\n","        while True:\n","            count+=1\n","            # determine which agent to take to select the action according to the current number of drying vehicles\n","            if state_do[17]<=5:\n","                action_do_1 = agent_dosage.selection_action(state_do)  # generate actions, where state is the input and action is the output\n","            else:\n","                action_do_1 = agent_do.selection_action(state_do)  # generate actions, where state is the input and action is the output\n","                \n","            if transfer:\n","                noise = max(0.2 - 0.01 * epoch, 0.0)\n","            else:\n","                noise = max(0.2 - 0.0004 * epoch, 0.0)\n","\n","            action_do_1 = (action_do_1 + np.random.normal(0, noise, size=17)).clip(0.0, 1.0) # add noise for exploration. Keep it between 0 and 1\n","            # transition\n","            state_do=np.array(state_do)\n","            X=scaler_std1.transform(state_do.reshape(1, -1))# Normalize the values\n","            train_scaled = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n","            # Get the emission of pollutants at the current moment\n","            y_NMHC=model_NMHC.predict(train_scaled)[0]\n","            y_NOx=model_NOx.predict(train_scaled)[0]\n","            \n","            y_std= np.append(y_NMHC,y_NOx, axis=0)# The two pollutants are discharged together\n","            y=scaler_std2.inverse_transform(y_std.reshape(1, -1))\n","            next_state_1, reward_1, done_1, info_1 = env.step(action_do_1,y,count,current_car=state_do[17])\n","            next_state_do = next_state_1\n","            \n","            ep_r += reward_1\n","            ##### determine which agent's buffer to add data to, based on the actual number of vehicles\n","            if state_do[17]<=5:\n","                agent_dosage.replay_buffer.push(\n","                    (state_do, next_state_do,action_do_1,reward_1, np.float(done_1)))\n","            else:\n","                agent_do.replay_buffer.push(\n","                    (state_do, next_state_do,action_do_1,reward_1, np.float(done_1)))                \n","\n","            state_do = next_state_do\n","            t += 1\n","            if t >= args.max_length_of_trajectory:# Is this trajectory a total of how many movements, more than you can stop\n","                if epoch % args.print_log == 0:\n","                    print(\"Ep_i \\t{}, the ep_r is \\t{:0.2f}, the step is \\t{}\".format(epoch, ep_r / t, t))\n","                episode_reward.append(ep_r / t)\n","                ep_r = 0\n","                ep_a=[]\n","                save_output(episode_reward=episode_reward, save_path='reward/ddpg_final{}.txt'.format(i, epoch))\n","                break\n","\n","        # update network\n","        if len(agent_do.replay_buffer.storage) >= args.capacity - 1:\n","            agent_do.update(agent_do)      \n","            torch.save(agent_do.actor, 'model/actor_do.pkl')\n","            torch.save(agent_do.critic, 'model/critic_do.pkl')\n","            save_output(episode_reward=episode_reward, save_path='reward/ddpg_{}_{}.txt'.format(i, phase))\n","            #save_output(episode_reward=episode_aer, save_path='aer/aer_{}_{}.txt'.format(i, phase))\n","        \n","        # When the number of vehicles is small, the sample size is relatively small, so the two agents are updated separately\n","        if len(agent_dosage.replay_buffer.storage) >= args.capacity2 - 1:\n","            agent_dosage.update(agent_dosage)\n","            torch.save(agent_dosage.actor, 'model/actor_dosage.pkl')\n","            torch.save(agent_dosage.critic, 'model/critic_dosage.pkl')\n","        \n","if __name__ == '__main__':\n","    num = 1\n","    phase = 2\n","    for i in range(num):\n","        print('Recursion {}'.format(i))\n","        main(i)\n","\n","    #df_reward = concatenate_data('reward/', num=num*args.max_episode, algorithm='DDPG')\n","    #df_aer = concatenate_data('aer/', num=num*args.max_episode, algorithm='aer')\n","    #reward_plot(data=df_reward, values=['DDPG'], ylabel='Reward')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZmfsI8_Pxcw"},"outputs":[],"source":["hhhhhhhhhhhhhh\n","jjjj\n","kkk"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"DDPG_pollution_final_final.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
